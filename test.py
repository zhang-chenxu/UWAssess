import argparse
import os
import cv2
import numpy as np
from PIL import Image
import torch
import yaml
import shutil
from torchvision import transforms
from models_sam2 import sam_semi_stochasticdepth_LoraAdapterexternal as sam_model

from argparse import ArgumentParser
import torch
from transformers import AutoModelForCausalLM
import json
from deepseek_vl2.models import DeepseekVLV2ForCausalLM, DeepseekVLV2Processor
from deepseek_vl2.serve.app_modules.utils import parse_ref_bbox
from deepseek_vl2.utils.io import load_pil_images


def get_response(conversation, chunk_size):
    pil_images = load_pil_images(conversation)
    prepare_inputs = vl_chat_processor.__call__(
        conversations=conversation,
        images=pil_images,
        force_batchify=True,
        system_prompt=""
    ).to(vl_gpt.device, dtype=deepseek_dtype)

    with torch.no_grad():
        if chunk_size == -1:
            inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)
            past_key_values = None
        else:
            # incremental_prefilling when using 40G GPU for vl2-small
            inputs_embeds, past_key_values = vl_gpt.incremental_prefilling(
                input_ids=prepare_inputs.input_ids,
                images=prepare_inputs.images,
                images_seq_mask=prepare_inputs.images_seq_mask,
                images_spatial_crop=prepare_inputs.images_spatial_crop,
                attention_mask=prepare_inputs.attention_mask,
                chunk_size=chunk_size
            )

        outputs = vl_gpt.generate(
            inputs_embeds=inputs_embeds,
            input_ids=prepare_inputs.input_ids,
            images=prepare_inputs.images,
            images_seq_mask=prepare_inputs.images_seq_mask,
            images_spatial_crop=prepare_inputs.images_spatial_crop,
            attention_mask=prepare_inputs.attention_mask,
            past_key_values=past_key_values,
            pad_token_id=tokenizer.eos_token_id,
            bos_token_id=tokenizer.bos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            max_new_tokens=512,
            do_sample=False,
            use_cache=True,
        )

        result = tokenizer.decode(outputs[0][len(prepare_inputs.input_ids[0]):].cpu().tolist(), skip_special_tokens=False)

    return result[:-19]


def deepseek_eval(input_image_path, visual_perception_output_path, chunk_size):

    # ### caption generation
    image_prompt = "Image: <image>\n\n"
    task_prompt1 = "Task: Generate a concise caption for the image, including key information such as weather, lighting, road conditions, and surrounding objects."
    total_prompt1 = image_prompt + task_prompt1
    conversation = [
        {
            "role": "<|User|>",
            "content": total_prompt1,
            "images": [
                input_image_path
            ],
        },
        {"role": "<|Assistant|>", "content": ""},
    ]

    caption = get_response(conversation, chunk_size)

    # ### report generation
    caption_prompt = "Caption: " + caption + "\n\n"
    visual_prompt = "Visual perception result: <image>\nThis is generated by a waterlogging perception model, which highlights potential waterlogged areas in red over the original image.\n\n"
    task_prompt2 = "Task: Determine whether the original image contains waterlogging by comprehensively analyzing the key information in the caption and the visual perception result. If there are no waterlogged areas in the image, output 'There is no obvious waterlogged area in the image.' Otherwise, generate a structured waterlogging assessment report for the original image with the following three sections: \n1. Extent Assessment: Describe the spatial coverage and distribution of the waterlogged area. \n2. Depth Assessment: Comprehensively describe the relative water depth based on visible surrounding reference objects such as car tires, curbs, or pedestrians. \n3. Risk and Impact Assessment: Comprehensively list and assess the possible impacts and risks of waterlogging on traffic flow, pedestrian movement, vehicle safety, public facilities, etc."
    total_prompt2 = image_prompt + caption_prompt + visual_prompt + task_prompt2
    conversation = [
        {
            "role": "<|User|>",
            "content": total_prompt2,
            "images": [
                input_image_path,
                visual_perception_output_path
            ],
        },
        {"role": "<|Assistant|>", "content": ""},
    ]

    report = get_response(conversation, chunk_size)

    report_data = {
        "image": input_image_path.split('/')[-1],
        "caption": [
            {
                "from": "UWAssess",
                "value": caption
            }
        ],
        "report": [
            {
                "from": "UWAssess",
                "value": report
            }
        ]
    }

    return report_data


def draw_mask(image, mask_generated):
    masked_image = image.copy()

    masked_image = np.where(mask_generated.astype(int),
                          np.array([255, 0, 0], dtype='uint8'),
                          masked_image)

    masked_image = masked_image.astype(np.uint8)

    return cv2.addWeighted(image, 0.7, masked_image, 0.3, 0)


def uwassess_eval(test_path, save_path):
    for image_name in os.listdir(test_path):
        input_image_path = os.path.join(test_path, image_name)
        input_image = Image.open(input_image_path).convert('RGB')
        inp = transforms.Compose([
            transforms.Resize((model_args['inp_size'], model_args['inp_size'])),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])(input_image)
        with torch.no_grad():
            # ## visual perception
            inp = inp.to(device).unsqueeze(0)
            pred = model(inp)
            pred = transforms.functional.resize(pred, (input_image.size[1], input_image.size[0]),
                                                transforms.InterpolationMode.BILINEAR)
            pred = torch.sigmoid(pred)
            pred = pred.squeeze().detach().cpu().numpy()
            pred = np.where(pred > 0.5, 1, 0)

            mask = np.array([pred for i in range(3)]).transpose(1, 2, 0)
            show_image = draw_mask(np.array(input_image), mask)
            visual_root, _ = os.path.splitext(os.path.join(perception_save_path, image_name))
            output_image_path = visual_root + '.png'
            save_image = cv2.cvtColor(show_image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(output_image_path, save_image)


            # ## textual report generation
            textual_report = deepseek_eval(input_image_path, output_image_path, args.chunk_size)
            textual_root, _ = os.path.splitext(os.path.join(report_save_path, image_name))
            output_report_path = textual_root + '.json'
            with open(output_report_path, 'w') as file:
                json.dump(textual_report, file, indent=2)


parser = argparse.ArgumentParser()
parser.add_argument('--test_path', default='./test_set/RoadwayFlooding/JPEGImages', help='path to test images')
parser.add_argument('--save_path', default='./results', help='path to save results')
parser.add_argument('--vision_config', default='./configs/sam2_configs/cod-sam-vit-s-semi_30epoch.yaml', help='path to sam2 model configs')
parser.add_argument('--vision_checkpoint', default='./checkpoints/uwassess_vision.pth', help='path to adapted sam2 model')
parser.add_argument('--deepseek_model_path', default='./deepseek-vl2-small', help='path to deepseek-vl2 model')
parser.add_argument('--device', type=str, default='cuda:0')
parser.add_argument("--chunk_size", type=int, default=-1,
                    help="chunk size for the model for prefiiling. "
                         "When using 40G gpu for vl2-small, set a chunk_size for incremental_prefilling."
                         "Otherwise, default value is -1, which means we do not use incremental_prefilling.")
args = parser.parse_args()


# Adapted SAM2 deployment
device = torch.device(args.device)
with open(args.vision_config, 'r') as f:
    config = yaml.load(f, Loader=yaml.FullLoader)
model_args = config['model']['args']
model = sam_model.SAM(inp_size=model_args['inp_size'], encoder_mode=model_args['encoder_mode'])
model = model.to(device)
checkpoint = torch.load(args.vision_checkpoint, map_location=device)
model.load_state_dict(checkpoint)
model.eval()


# DeepSeek-VL2 deployment
deepseek_dtype = torch.bfloat16
vl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(args.deepseek_model_path)
tokenizer = vl_chat_processor.tokenizer
vl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(
    args.deepseek_model_path,
    trust_remote_code=True,
    torch_dtype=deepseek_dtype)
vl_gpt = vl_gpt.to(device).eval()

perception_save_path = os.path.join(args.save_path, 'visual_perception')
if not os.path.exists(perception_save_path):
    os.makedirs(perception_save_path)
report_save_path = os.path.join(args.save_path, 'textual_report')
if not os.path.exists(report_save_path):
    os.makedirs(report_save_path)
uwassess_eval(args.test_path, args.save_path)
